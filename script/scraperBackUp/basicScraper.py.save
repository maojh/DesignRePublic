
import requests, re
from bs4 import BeautifulSoup
import csv
import sys
import glob, os

print "starting"
reload(sys)
sys.setdefaultencoding('utf8')

#r = requests.get("https://re.public.polimi.it/handle/11311/1019072?mode=full")
#if r is not None:
#	print "request gone wrong"
#html = "".join(line.strip() for line in r.content.split("\n"))
#soup = BeautifulSoup(html, 'lxml')

#rows = soup.find_all('tr')


content = []
headers = []
values = []

with open("pubLink_clean.csv","rb") as f:
	reader = csv.reader(f)
	next(reader, None)
	for pub in reader:
		r = requests.get( pub[0].join('?mode=full'))
		print r
		html = "".join(line.strip() for line in r.content.split("\n"))
			soup = BeautifulSoup(html,'lxml')
		print soup.find_all('tr')[0].td.string
		break




#	for i in range(1, len(rows) -5):	#ignore last 3 field, they are file link
#		c = rows[i].contents
#		head = c[0].string
#		headers.append( c[0].string )

#	for j in range(1, len(rows) -5):
#		c = rows[j].contents
#		val = c[1].string
#		val.replace(" ","")
#		values.append( c[1].string )

#	content.append([headers,values])
	#print values

#with open('scrapingSingolo3.csv','wb') as fw:
#	writer = csv.writer(fw,delimiter=",")
#	writer.writerow(headers)
#	writer.writerow(values)

#with open('scrapingSingolo.csv','rb') as fr:
#	reader = csv.reader(fr)
#	for row in reader:
#		print row

